<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners (your "business card") -->
  <meta name="description" content="ICLR 2026 project page for Preserve and Personalize: Personalized Text-to-Image Diffusion Models without Distributional Drift.">
  <meta property="og:title" content="Preserve and Personalize (ICLR 2026)"/>
  <meta property="og:description" content="Personalized text-to-image diffusion without distributional drift via a Lipschitz-regularized objective that guarantees preservation of the pretrained distribution."/>
  <meta property="og:url" content="https://rlgnswk.github.io/PnP_ProjectPage/"/>
  <!-- Path to banner image (optimal 1200x630) -->
  <meta property="og:image" content="static/pnp_toy.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="Preserve and Personalize (ICLR 2026)">
  <meta name="twitter:description" content="Personalized text-to-image diffusion without distributional drift via Lipschitz regularization.">
  <!-- Path to banner image (optimal 1200x600) -->
  <meta name="twitter:image" content="static/pnp_toy.png">
  <meta name="twitter:card" content="summary_large_image">

  <!-- Keywords for indexing -->
  <meta name="keywords" content="text-to-image diffusion, personalization, distributional drift, preservation, Lipschitz regularization, ICLR 2026">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Preserve_and_Personalize</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<style>
  .container { text-align: center; }
  .item { display: inline-block; margin: auto; }
  .item video { width: 1024px; height: 1024px; max-width: 100%; }
  .small-text { font-size: 20px; }
</style>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">

          <h1 class="title is-1 publication-title">
            Preserve and Personalize: Personalized Text-to-Image Diffusion Models without Distributional Drift
          </h1>

          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=YOUR_GIHOON_SCHOLAR_ID" target="_blank">Gihoon Kim</a>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=YOUR_HYUNGJIN_SCHOLAR_ID" target="_blank">Hyungjin Park</a>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=YOUR_TAESUP_SCHOLAR_ID" target="_blank">Taesup Kim</a></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://iclr.cc/" target="_blank">ICLR 2026</a><br>
            </span>
            <span class="eql-cntrb">
              <small><br>Seoul National University</small>
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- Paper link -->
              <span class="link-block">
                <a href="PAPER_URL" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Code link -->
              <span class="link-block">
                <a href="CODE_URL" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>

              <!-- Supplementary link -->
              <span class="link-block">
                <a href="SUPPLEMENTARY_URL" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Supplementary</span>
                </a>
              </span>

              <!-- Optional: Poster link -->
              <span class="link-block">
                <a href="POSTER_URL" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-image"></i></span>
                  <span>Poster</span>
                </a>
              </span>

            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- TL;DR -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            <b>TL;DR:</b> We propose a simple yet effective objective grounded in Lipschitz regularization that guarantees preservation of the pretrained distribution, which prior approaches failed to ensure.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Single image -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Figure</h2>
      <div class="item">
        <img src="static/pnp_toy.png" alt="pnp toy figure" style="max-width: 100%; height: auto;">
      </div>
    </div>
  </div>
</section>
<!-- End single image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Personalizing text-to-image diffusion models involves integrating novel visual concepts
            from a small set of reference images while retaining the model’s original generative capabilities.
            However, this process often leads to overfitting, where the model ignores the user’s prompt and
            merely replicates the reference images. We attribute this issue to a fundamental misalignment
            between the true goals of personalization, which are subject fidelity and text alignment, and the
            training objectives of existing methods that fail to enforce both objectives simultaneously.
            Specifically, prior approaches often overlook the need to explicitly preserve the pretrained
            model’s output distribution, resulting in distributional drift that undermines diversity and coherence.
            To resolve these challenges, we introduce a Lipschitz-based regularization objective that constrains
            parameter updates during personalization, ensuring bounded deviation from the original distribution.
            This promotes consistency with the pretrained model’s behavior while enabling accurate adaptation to
            new concepts. Furthermore, our method offers a computationally efficient alternative to commonly used,
            resource-intensive sampling techniques. Through extensive experiments across diverse diffusion model
            architectures, we demonstrate that our approach achieves superior performance in both quantitative
            metrics and qualitative evaluations, consistently excelling in visual fidelity and prompt adherence.
            We further support these findings with comprehensive analyses, including ablation studies and
            visualizations.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel (kept as-is) -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Figures</h2>
      <div class="carousel results-carousel">
        <div class="item">
          <img src="static/pnp_fig4.png" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Teaser: Our approach, NeRFFaceSpeech, constructs a 3D-aware facial feature space from a single image utilizing generative priors and incorporates audio-driven dynamics via ray deformation, enabling the synthesis of talking heads from novel viewpoints.
          </h2>
        </div>
        <div class="item">
          <img src="static/pnp_fig3.png" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Overall pipeline of our method: Given a single image and an audio input, the preprocessing stage extracts 3DMM parameters and 3D features in the NeRF space. Subsequent spatial synchronization between 3DMM and feature space enables the reflection of expression vertex changes within the feature space. These changes are computed in accordance with the audio time step t, resulting in the generation of facial movements. The final output frame emerges from feature blending, where the deformed features are merged with the inner-mouth details generated by the proposed LipaintNet.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->

<!-- Single image -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Figure</h2>
      <div class="item">
        <img src="static/pnp_algo.png" alt="pnp toy figure" style="max-width: 100%; height: auto;">
      </div>
    </div>
  </div>
</section>
<!-- End single image -->

<!-- Image carousel (no captions) -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Figures</h2>
      <div class="carousel results-carousel">

        <div class="item">
          <img src="static/pnp_fig2.png" alt="pnp figure 1"/>
        </div>

        <div class="item">
          <img src="static/pnp_fig5.png" alt="pnp figure 2"/>
        </div>

        <div class="item">
          <img src="static/pnp_fig6.png" alt="pnp figure 3"/>
        </div>

      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->


<!-- BibTeX -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre style="text-align: left;"><code>
@inproceedings{kim2026preserveandpersonalize,
  title     = {Preserve and Personalize: Personalized Text-to-Image Diffusion Models without Distributional Drift},
  author    = {Gihoon Kim and Hyungjin Park and Taesup Kim},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2026}
}
    </code></pre>
  </div>
</section>
<!-- End BibTeX -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>
            which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br>
            This website is licensed under a
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
              Creative Commons Attribution-ShareAlike 4.0 International License
            </a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Optional: GitHub tag fetch (update repo URL) -->
<script>
  fetch('https://api.github.com/repos/rlgnswk/PnP_ProjectPage/tags')
    .then(response => response.json())
    .then(data => {
      if (!data || data.length === 0) return;
      var latestTag = data[0];
      var el = document.getElementById('gitTagInfo');
      if (el) el.innerHTML = `Latest Release: ${latestTag.name}`;
    })
    .catch(err => {
      // optional: silently ignore
      console.log('Failed to fetch tags:', err);
    });
</script>

</body>
</html>
